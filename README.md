# ğŸ™ï¸ H.E.L.E.N 2.0 - Voice AI Assistant

**H**uman **E**nhanced **L**istening & **E**ngagement **N**etwork

Una asistente de voz impulsada por IA con detecciÃ³n automÃ¡tica de voz (VAD), streaming en tiempo real y visualizaciÃ³n interactiva de audio.

![Version](https://img.shields.io/badge/version-2.0-blue.svg)
![Node](https://img.shields.io/badge/node-v20+-green.svg)
![License](https://img.shields.io/badge/license-MIT-orange.svg)

## âœ¨ CaracterÃ­sticas

### ğŸ¯ Core Features
- **ğŸ¤ VAD (Voice Activity Detection)**: DetecciÃ³n automÃ¡tica de voz sin necesidad de mantener botones presionados
- **ğŸ”„ Streaming Bidireccional**: WebSocket con Socket.IO para comunicaciÃ³n en tiempo real
- **ğŸ—£ï¸ ConversaciÃ³n Continua**: Flujo automÃ¡tico sin interrupciones manuales
- **ğŸµ Audio Fluido**: Sistema de buffering para reproducciÃ³n sin cortes

### ğŸ¤– IntegraciÃ³n de IA
- **Whisper (OpenAI)**: TranscripciÃ³n de voz a texto en espaÃ±ol
- **ChatGPT Assistants API**: ConversaciÃ³n inteligente con contexto
- **ElevenLabs**: Text-to-Speech con voz multilingÃ¼e de alta calidad

### ğŸ¨ Interfaz Visual
- **Esfera 3D Interactiva**: Animada con GSAP
- **Sistema de PartÃ­culas**: Reacciona al audio en tiempo real
- **AudioVisualizer**: VisualizaciÃ³n espectral del audio
- **Animaciones Fluidas**: transiciones suaves con GSAP

## ğŸš€ Inicio RÃ¡pido

### Prerequisitos

```bash
node >= 20.0.0
npm >= 9.0.0
```

### InstalaciÃ³n

1. **Clonar el repositorio:**
```bash
git clone https://github.com/AndreSaul16/H.E.L.E.N.git
cd H.E.L.E.N
```

2. **Instalar dependencias:**
```bash
npm install
```

3. **Configurar variables de entorno:**

Crea un archivo `.env` en la raÃ­z del proyecto:

```env
# OpenAI (Whisper + ChatGPT)
OPENAI_API_KEY=sk-...
OPENAI_ASSISTANT_ID=asst_...

# ElevenLabs (TTS)
ELEVENLABS_API_KEY=...
ELEVENLABS_VOICE_ID=qHkrJuifPpn95wK3rm2A

# Server Config
PORT=4000
```

### Ejecutar

**Modo desarrollo (backend + frontend):**
```bash
npm run dev:full
```

O ejecutar por separado:

```bash
# Terminal 1 - Backend
npm run dev

# Terminal 2 - Frontend
npm run dev:vite
```

Abrir en el navegador: `http://localhost:5173`

## ğŸ“ Estructura del Proyecto

```
H.E.L.E.N-version-2.0-preview/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js                 # Servidor principal
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ socketHandler.js      # Manejo de WebSocket
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ WhisperService.js     # STT con OpenAI Whisper
â”‚       â”œâ”€â”€ ChatGPTService.js     # IA conversacional
â”‚       â””â”€â”€ ElevenLabsService.js  # TTS con streaming
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html                # PÃ¡gina principal
â”‚   â”œâ”€â”€ app.js                    # LÃ³gica principal + VAD
â”‚   â”œâ”€â”€ audioCapture.js           # Sistema VAD
â”‚   â”œâ”€â”€ audioVisualizer.js        # VisualizaciÃ³n de audio
â”‚   â”œâ”€â”€ particleSystem.js         # Sistema de partÃ­culas
â”‚   â””â”€â”€ styles.css                # Estilos
â”œâ”€â”€ .env                          # Variables de entorno (no incluido)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ® CÃ³mo Usar

1. **Iniciar conversaciÃ³n**: Click en la esfera central
2. **Hablar**: El VAD detecta automÃ¡ticamente cuando hablas
3. **Esperar respuesta**: La IA procesa y responde con voz
4. **Continuar**: La conversaciÃ³n continÃºa automÃ¡ticamente
5. **Detener**: Click nuevamente para finalizar

## ğŸ”§ TecnologÃ­as

### Frontend
- **Vite**: Build tool y dev server
- **GSAP**: Animaciones
- **Socket.IO Client**: WebSocket
- **@ricky0123/vad-web**: Voice Activity Detection
- **SimplexNoise**: Generador de ruido para partÃ­culas
- **Web Audio API**: Procesamiento y visualizaciÃ³n de audio

### Backend
- **Node.js + Express**: Servidor HTTP
- **Socket.IO**: WebSocket server
- **OpenAI SDK**: Whisper + GPT-4
- **ElevenLabs SDK**: Text-to-Speech
- **FormData + Blob**: Manejo de audio
- **Multer**: Procesamiento de archivos

## ğŸ“Š Flujo de la AplicaciÃ³n

```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant VAD
    participant Backend
    participant Whisper
    participant ChatGPT
    participant ElevenLabs

    User->>Frontend: Click en esfera
    Frontend->>VAD: Iniciar detecciÃ³n
    VAD->>User: MicrÃ³fono activo
    User->>VAD: Habla
    VAD->>VAD: Detecta voz
    VAD->>VAD: Detecta silencio >1s
    VAD->>Backend: EnvÃ­a audio (WebSocket)
    Backend->>Whisper: Transcribir
    Whisper-->>Backend: Texto
    Backend->>ChatGPT: Generar respuesta
    ChatGPT-->>Backend: Respuesta
    Backend->>ElevenLabs: Text-to-Speech (streaming)
    ElevenLabs-->>Backend: Audio chunks
    Backend->>Frontend: Stream audio (WebSocket)
    Frontend->>Frontend: Buffer + reproduce
    Frontend->>User: Audio fluido
    Frontend->>VAD: Vuelve a escuchar
```

## ğŸ¯ CaracterÃ­sticas TÃ©cnicas Destacadas

### Voice Activity Detection (VAD)
- Modelo Silero VAD (ONNX)
- DetecciÃ³n en tiempo real (frames de 30ms)
- Threshold configurable: 0.5
- Silence detection: 1000ms
- Automatic commit on silence

### Audio Streaming
- **Buffering inteligente**: Combina chunks pequeÃ±os (>1000 bytes)
- **ReproducciÃ³n continua**: Un solo AudioBuffer para evitar gaps
- **VisualizaciÃ³n reactiva**: PartÃ­culas sincronizadas con audio
- **Latencia reducida**: ~9 segundos hasta primer audio

### WebSocket Architecture
- **Eventos bidireccionales**:
  - `audio-data`: Cliente â†’ Servidor (audio capturado)
  - `transcript`: Servidor â†’ Cliente (texto transcrito)
  - `response`: Servidor â†’ Cliente (respuesta de IA)
  - `audio-chunk`: Servidor â†’ Cliente (chunks de TTS)
  - `audio-end`: Servidor â†’ Cliente (fin de streaming)
  - `status`: Servidor â†’ Cliente (estados del proceso)

## ğŸ› Debugging

El proyecto incluye logging extensivo. Abre la consola del navegador (F12) para ver:

```
ğŸ™ï¸ [VAD] - Voice Activity Detection
ğŸ“¦ [WEBSOCKET] - Eventos de WebSocket
â–¶ï¸ [PLAYBACK] - ReproducciÃ³n de audio
ğŸ“¤ [SEND] - EnvÃ­o de datos
ğŸ¼ - DecodificaciÃ³n de audio
```

## ğŸ“ Notas

- **Requisito de HTTPS**: Para producciÃ³n, WebRTC/getUserMedia requiere HTTPS
- **Permisos de micrÃ³fono**: El navegador solicitarÃ¡ permiso en el primer uso
- **Costo de APIs**: Ten en cuenta los costos de OpenAI y ElevenLabs
- **Browser compatibility**: Chrome/Edge recomendados (Web Audio API + WASM)

## ğŸ¤ Contribuir

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una branch (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add: Amazing Feature'`)
4. Push a la branch (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“œ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT.

## ğŸ‘¤ Autor

**AndrÃ© SaÃºl**
- GitHub: [@AndreSaul16](https://github.com/AndreSaul16)

## ğŸ™ Agradecimientos

- OpenAI por Whisper y GPT-4
- ElevenLabs por su API de TTS
- @ricky0123 por la librerÃ­a VAD
- GreenSock (GSAP) por las animaciones

---

â­ Si este proyecto te ha sido Ãºtil, considera darle una estrella en GitHub!
